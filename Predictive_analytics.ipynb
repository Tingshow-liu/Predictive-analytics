{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code of predictive analytics ML pipeline in AI digital marketing project for company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get API keys from GitHub\n",
    "Complete this step for \"cloud automation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Username of team GitHub account\n",
    "username = '<Team GitHub account username>'\n",
    "\n",
    "# Personal Access Token (PAO) from team GitHub account\n",
    "token = '<Team GitHub account PAO>'\n",
    "\n",
    "github_session = requests.Session()\n",
    "github_session.auth = (username, token)\n",
    "\n",
    "# Key1: Google Bigquery query key\n",
    "url1 = '<URL to request API key1>' \n",
    "key1 = github_session.get(url1).json()\n",
    "\n",
    "with open(\"key1.json\", \"w\") as key1_file:  # Save as json file\n",
    "    json.dump(key1, key1_file)\n",
    "\n",
    "# Key2: GA management API key\n",
    "url2 = '<URL to request API key2>'\n",
    "key2 = github_session.get(url2).json()\n",
    "\n",
    "with open(\"key2.json\", \"w\") as key2_file: \n",
    "    json.dump(key2, key2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from openpyxl import Workbook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to company data stored in Google BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect Google BigQuery with Python\n",
    "import os\n",
    "from google.cloud import bigquery as bq\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'key1.json'  # Query key from GitHub (json file)\n",
    "\n",
    "# client = bq.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update realtime website behavior data in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Update_data:\n",
    "\n",
    "    def __init__(self, table):\n",
    "        self.table = table  # company data source on BigQuery\n",
    "        self.client = bq.Client()\n",
    "\n",
    "\n",
    "    # Update raw data\n",
    "    def update_raw(self):\n",
    "        \n",
    "        update_raw_query = self.client.query(\n",
    "        f'''\n",
    "        CREATE OR REPLACE TABLE \n",
    "        `<company data table on BigQuery: self.table>` AS\n",
    "\n",
    "        SELECT *\n",
    "        FROM `<company data view on BigQuery>`\n",
    "        WHERE \n",
    "        _table_suffix BETWEEN FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 11 DAY))\n",
    "        AND FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY))\n",
    "        '''   \n",
    "        )\n",
    "\n",
    "        results = update_raw_query.result()     \n",
    "\n",
    "        for row in results:\n",
    "            print(\"{} : {} views\".format(row.url, row.view_count))\n",
    "\n",
    "\n",
    "    \n",
    "    # \"Sample code\" of company \"customer website behavior data\" for prediction \n",
    "    # Generate ML features from \"Google Analytics raw data\" (total feautures generated: about 120)\n",
    "    def update_pivot(self):\n",
    "\n",
    "        update_pivot_query = self.client.query(\n",
    "        f'''\n",
    "        CREATE OR REPLACE TABLE \n",
    "        `<company data table on BigQuery>` AS(\n",
    "\n",
    "        WITH\n",
    "\n",
    "        -- Generate data range\n",
    "            input_date_array AS(\n",
    "            SELECT date\n",
    "            FROM UNNEST(GENERATE_DATE_ARRAY('2022-10-01', '2022-12-31')) AS date  # campaign period\n",
    "            ),\n",
    "\n",
    "            date_array AS(\n",
    "            SELECT date AS snapshot_date  \n",
    "            FROM input_date_array\n",
    "            WHERE date = DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)\n",
    "            ),\n",
    "\n",
    "\n",
    "        -- Feature 1\n",
    "            feat1_relabel AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                hits.eventInfo.eventAction AS event_page,\n",
    "                hits.eventInfo.eventCategory AS event_name,\n",
    "                date AS event_date,\n",
    "                CASE\n",
    "                WHEN hits.eventInfo.eventLabel = \"<customer behavior name>\" THEN 1\n",
    "                WHEN hits.eventInfo.eventLabel = \"<customer behavior name>\" THEN 2\n",
    "                WHEN hits.eventInfo.eventLabel = \"<customer behavior name>\" THEN 3\n",
    "                WHEN hits.eventInfo.eventLabel = \"<customer behavior name>\" THEN 4\n",
    "                ELSE\n",
    "                0\n",
    "            END\n",
    "                AS feat1_score\n",
    "            FROM\n",
    "                `<company data table on BigQuery: self.table>`,\n",
    "                UNNEST (hits) AS hits\n",
    "            WHERE\n",
    "                hits.eventInfo.eventCategory=\"<customer behavior category>\"),\n",
    "\n",
    "            feat1_score AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                event_date,\n",
    "                event_page,\n",
    "                event_name,\n",
    "                MAX(feat1_score) AS max_feat1_score\n",
    "            FROM\n",
    "                feat1_relabel\n",
    "            GROUP BY 1,2,3,4,5\n",
    "            ORDER BY 1,2,3,4\n",
    "            ),\n",
    "\n",
    "            feat1_aggregated_score AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                event_date,\n",
    "                event_name,\n",
    "                SUM(max_feat1_score) as feat1_score\n",
    "            FROM\n",
    "            feat1_score \n",
    "            GROUP BY 1,2,3,4\n",
    "            ORDER BY 1,2,3,4\n",
    "            ),\n",
    "\n",
    "\n",
    "        -- Feature 2\n",
    "            feat2_relabel AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                hits.eventInfo.eventCategory as event_name,\n",
    "                date AS event_date,\n",
    "                CASE\n",
    "                WHEN hits.eventInfo.eventLabel = \"<customer behavior name>\" THEN 1\n",
    "                WHEN hits.eventInfo.eventLabel = \"<customer behavior name>\" THEN 2\n",
    "                ELSE\n",
    "                0\n",
    "            END\n",
    "                AS feat2_score\n",
    "            FROM\n",
    "            `<company data table on BigQuery: self.table>`,\n",
    "            UNNEST (hits) AS hits \n",
    "            WHERE\n",
    "            hits.eventInfo.eventCategory=\"<customer behavior category>\"\n",
    "            AND hits.eventInfo.eventAction != \"<customer behavior name>\"),\n",
    "\n",
    "            feat2_aggregated_score AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                event_date,\n",
    "                event_name,\n",
    "                SUM(feat2_score) AS sum_feat2_score\n",
    "            FROM\n",
    "            feat2_relabel\n",
    "            GROUP BY 1,2,3,4\n",
    "            ORDER BY 1,2,3,4\n",
    "            ),\n",
    "\n",
    "            \n",
    "        -- Feature 3\n",
    "            feat3_relabel AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                hits.eventInfo.eventAction as event_link,\n",
    "                hits.eventInfo.eventCategory as event_name,\n",
    "                date AS event_date,\n",
    "                CASE\n",
    "                WHEN hits.eventInfo.eventLabel = \"<customer behavior name>\" THEN 1\n",
    "                WHEN hits.eventInfo.eventLabel = \"<customer behavior name>\" THEN 2\n",
    "                WHEN hits.eventInfo.eventLabel = \"<customer behavior name>\" THEN 3\n",
    "                WHEN hits.eventInfo.eventLabel = \"<customer behavior name>\" THEN 4\n",
    "                WHEN hits.eventInfo.eventLabel = \"<customer behavior name>\" THEN 5\n",
    "                ELSE\n",
    "                0\n",
    "            END\n",
    "                AS feat3_score\n",
    "            FROM\n",
    "                `<company data table on BigQuery: self.table>`,\n",
    "                UNNEST (hits) AS hits\n",
    "            WHERE\n",
    "                hits.eventInfo.eventCategory=\"<customer behavior category>\" ),\n",
    "\n",
    "            feat3_score AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                event_date,\n",
    "                event_link,\n",
    "                event_name,\n",
    "                MAX(feat3_score) AS max_feat3_score\n",
    "            FROM\n",
    "                feat3_relabel\n",
    "            GROUP BY 1,2,3,4,5\n",
    "            ORDER BY 1,2,3,4\n",
    "            ),\n",
    "\n",
    "            feat3_aggregated_score AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                event_date,\n",
    "                event_name,\n",
    "                SUM(max_feat3_score) as feat3_score\n",
    "            FROM\n",
    "            feat3_score \n",
    "            GROUP BY 1,2,3,4\n",
    "            ORDER BY 1,2,3,4\n",
    "            ),\n",
    "\n",
    "\n",
    "        -- Other features: different \"button clicks\" on company website (sample code)\n",
    "\n",
    "            -- Count total clicks of buttons: only remain important buttons\n",
    "            count_clicks AS(\n",
    "            SELECT\n",
    "                hits.eventInfo.eventCategory AS event_category,\n",
    "                hits.eventInfo.eventLabel AS event_label,\n",
    "                COUNT(hits.eventInfo.eventCategory) AS click_count\n",
    "            FROM \n",
    "                `<company data table on BigQuery: self.table>`,\n",
    "                UNNEST(hits) AS hits\n",
    "            WHERE hits.eventInfo.eventCategory LIKE '%<customer behavior category>%'\n",
    "            GROUP BY 1,2\n",
    "            ORDER BY 3 DESC\n",
    "            ),\n",
    "\n",
    "            -- Filter\n",
    "            button_names AS(\n",
    "            SELECT \n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                date AS event_date,\n",
    "                hits.eventInfo.eventCategory AS event_category,\n",
    "                hits.eventInfo.eventLabel AS event_label,\n",
    "                CONCAT(hits.eventInfo.eventCategory, \"-\", hits.eventInfo.eventLabel) AS button_name,  \n",
    "            FROM \n",
    "                `<company data table on BigQuery: self.table>`,\n",
    "                UNNEST (hits) AS hits\n",
    "            WHERE \n",
    "                hits.eventInfo.eventCategory LIKE '%<customer behavior category>%'\n",
    "            ),\n",
    "\n",
    "            -- Join\n",
    "            buttons_joined AS(\n",
    "            SELECT \n",
    "                button_names.clientId, \n",
    "                button_names.visitNumber, \n",
    "                button_names.event_date, \n",
    "                button_names.button_name, \n",
    "                count_clicks.click_count\n",
    "            FROM button_names\n",
    "            INNER JOIN count_clicks ON button_names.event_category = count_clicks.event_category\n",
    "            AND button_names.event_label = count_clicks.event_label\n",
    "            ),\n",
    "\n",
    "            buttons_relabel AS(\n",
    "            SELECT \n",
    "                clientId, \n",
    "                visitNumber,\n",
    "                event_date,\n",
    "                CASE         \n",
    "                    WHEN button_name IN ('<important company website buttons to keep their names>') THEN button_name\n",
    "                    ELSE 'buttons_other'  -- Other buttons\n",
    "                END \n",
    "                    AS event_name,\n",
    "                CASE\n",
    "                    WHEN event_name IN (<the most important company website buttons>) THEN 3\n",
    "                    WHEN event_name IN (<important company website buttons>) THEN 2\n",
    "                    ELSE 1 \n",
    "                END \n",
    "                    AS buttons_score  -- Other buttons\n",
    "            FROM buttons_joined\n",
    "            WHERE\n",
    "                click_count >= 200  -- At least 200 clicks\n",
    "            ),\n",
    "\n",
    "            buttons_aggregated_score AS(\n",
    "            SELECT \n",
    "                clientId,\n",
    "                visitNumber, \n",
    "                event_date,\n",
    "                event_name,\n",
    "                SUM(buttons_score) AS buttons_score \n",
    "            FROM buttons_relabel\n",
    "            GROUP BY 1,2,3,4\n",
    "            ORDER BY 1,2,3,4\n",
    "            ),\n",
    "\n",
    "\n",
    "        -- Skip rest of the feature creation code, which are similar to the code above \n",
    "\n",
    "\n",
    "        -- y (purchase)\n",
    "            label_y AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                date AS event_date,\n",
    "                \"label_transaction\" as event_name,\n",
    "                CASE\n",
    "                WHEN totals.transactions is NULL THEN 0\n",
    "                ELSE\n",
    "                    1\n",
    "                END\n",
    "                    AS transaction\n",
    "            FROM\n",
    "                `<company data table on BigQuery: self.table>`\n",
    "                ORDER BY 1,2,3\n",
    "            ),\n",
    "\n",
    "\n",
    "        -- Union all features created\n",
    "            feature AS(\n",
    "                SELECT * FROM feat1_aggregated_score\n",
    "                UNION ALL\n",
    "                SELECT * FROM feat2_aggregated_score\n",
    "                UNION ALL\n",
    "                SELECT * FROM feat3_aggregated_score\n",
    "                UNION ALL\n",
    "                SELECT * FROM buttons_aggregated_score \n",
    "                -- Other features skipped\n",
    "            ),\n",
    "\n",
    "\n",
    "        -- Cross Join with date\n",
    "            feature_cross_join AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                event_date,\n",
    "                snapshot_date,\n",
    "                REGEXP_REPLACE(replace(replace(replace(replace(replace(replace(replace(event_name,\" \",\"_\"),\"-\",\"_\"),\"/\",\"_\"),\"ó\",\"o\"),\"í\",\"i\"),\"é\",\"e\"),\"á\",\"a\"), r\"[^a-zA-Z0-9]+\", \"_\") as event_name,\n",
    "                SUM(IF(DATE_DIFF(CAST(CONCAT(SUBSTRING(event_date,1,4),\"-\",SUBSTRING(event_date,5,2),\"-\",SUBSTRING(event_date,7,2)) as date), snapshot_date,Day) Between -10 and 0, session_scroll_score,0)) AS time_windowed_metrics\n",
    "            FROM\n",
    "                feature\n",
    "            CROSS JOIN\n",
    "                date_array\n",
    "            GROUP BY \n",
    "                1,2,3,4,5\n",
    "            ),\n",
    "\n",
    "            label_cross_join AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                event_date,\n",
    "                snapshot_date,\n",
    "                REGEXP_REPLACE(replace(replace(replace(replace(replace(replace(replace(event_name,\" \",\"_\"),\"-\",\"_\"),\"/\",\"_\"),\"ó\",\"o\"),\"í\",\"i\"),\"é\",\"e\"),\"á\",\"a\"), r\"[^a-zA-Z0-9]+\", \"_\") as event_name,\n",
    "                SUM(IF(DATE_DIFF(CAST(CONCAT(SUBSTRING(event_date,1,4),\"-\",SUBSTRING(event_date,5,2),\"-\",SUBSTRING(event_date,7,2)) as date),snapshot_date,Day) Between 0 and 3, transaction,0)) AS time_windowed_metrics\n",
    "            FROM\n",
    "                label_y\n",
    "            CROSS JOIN\n",
    "                date_array\n",
    "            GROUP BY \n",
    "                1,2,3,4,5\n",
    "            ),\n",
    "\n",
    "            all_cross_join AS(\n",
    "                SELECT * FROM feature_cross_join\n",
    "                UNION ALL\n",
    "                SELECT * FROM label_cross_join\n",
    "            ),\n",
    "\n",
    "\n",
    "        -- Create pivot table (feature)\n",
    "            all_cross_join_pivot AS (\n",
    "\n",
    "                SELECT * FROM all_cross_join\n",
    "                \n",
    "                    PIVOT (\n",
    "                        SUM((time_windowed_metrics))\n",
    "                        -- Need support from python\n",
    "                        FOR event_name IN ('<all features generated above>')\n",
    "                )\n",
    "            ),\n",
    "\n",
    "\n",
    "        -- Static features\n",
    "            pre_static_feat AS(\n",
    "            SELECT\n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                trafficSource.keyword AS search_keyword,\n",
    "                device.browser AS browser,\n",
    "                device.isMobile AS is_mobile,\n",
    "                device.mobileDeviceBranding AS mobile_brand,\n",
    "                device.deviceCategory AS device_category,\n",
    "                geoNetwork.country AS country,\n",
    "                geoNetwork.region AS region,\n",
    "                IFNULL(totals.pageviews,0) as PageView,\n",
    "                IFNULL(totals.timeOnSite,0) as TimeOnSite,\n",
    "                CASE \n",
    "                WHEN CAST(hits.eCommerceAction.action_type AS int) = 2 THEN 1\n",
    "                ELSE 0 END AS view_product,  \n",
    "                CASE \n",
    "                WHEN CAST(hits.eCommerceAction.action_type AS int) = 3 THEN 1\n",
    "                ELSE 0 END AS add2cart, \n",
    "                CASE \n",
    "                WHEN CAST(hits.eCommerceAction.action_type AS int) = 5 THEN 1\n",
    "                ELSE 0 END AS checkout,  \n",
    "            FROM `<company data table on BigQuery: self.table>`,\n",
    "            UNNEST(hits) AS hits\n",
    "            ),\n",
    "\n",
    "            static_feat AS(\n",
    "            SELECT \n",
    "                clientId,\n",
    "                visitNumber,\n",
    "                search_keyword, \n",
    "                browser,\n",
    "                is_mobile,\n",
    "                mobile_brand,\n",
    "                device_category,\n",
    "                country,\n",
    "                region,\n",
    "                PageView,\n",
    "                timeOnSite,\n",
    "                SUM(view_product_or_not) AS view_amount,\n",
    "                SUM(add2cart_or_not) AS cart_amount,\n",
    "                SUM(checkout_or_not) AS checkout_amount,\n",
    "            FROM pre_static_feat\n",
    "            GROUP BY 1,2,3,4,5,6,7,8,9,10,11\n",
    "            ),\n",
    "\n",
    "\n",
    "        -- Merge all features\n",
    "            merged_table AS(\n",
    "            SELECT\n",
    "                all_cross_join_pivot.*, \n",
    "                static_feat.*\n",
    "            FROM \n",
    "                all_cross_join_pivot \n",
    "                JOIN static_feat ON all_cross_join_pivot.clientId = static_feat.clientId \n",
    "                AND all_cross_join_pivot.visitNumber = static_feat.visitNumber\n",
    "            )\n",
    "\n",
    "        SELECT * from merged_table\n",
    "        ORDER BY clientId, visitNumber, event_date, snapshot_date\n",
    "        )\n",
    "        '''\n",
    "        )\n",
    "        \n",
    "        results = update_pivot_query.result()     \n",
    "\n",
    "        for row in results:\n",
    "            print(\"{} : {} views\".format(row.url, row.view_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import BigQuery data to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries:\n",
    "\n",
    "    def __init__(self, static_dataset, behavior_dataset):\n",
    "        self.static_dataset = static_dataset\n",
    "        self.behavior_dataset = behavior_dataset\n",
    "        self.client = bq.Client()\n",
    "\n",
    "    # static query\n",
    "    def static_query(self):\n",
    "        static_query = f'''\n",
    "        WITH\n",
    "            pre_features AS(\n",
    "            SELECT \n",
    "                clientId,\n",
    "                PARSE_DATE('%Y%m%d', date) AS event_date,\n",
    "                visitNumber AS visit_number,\n",
    "                trafficSource.medium AS medium,\n",
    "                CASE WHEN CAST(hits.eCommerceAction.action_type AS int) = 6 THEN 1\n",
    "                ELSE 0 END AS purchase_or_not,  \n",
    "            FROM \n",
    "                `<company data table on BigQuery: self.table>`,\n",
    "                UNNEST(hits) AS hits\n",
    "            ),\n",
    "\n",
    "            x_features AS(\n",
    "            SELECT \n",
    "                clientId,\n",
    "                event_date,\n",
    "                visit_number,\n",
    "                medium,\n",
    "                SUM(purchase_or_not) AS purchase_amount,\n",
    "            FROM pre_features\n",
    "            GROUP BY 1,2,3,4\n",
    "            -- HAVING COUNT(clientId) > 1\n",
    "            )\n",
    "\n",
    "        SELECT * FROM x_features\n",
    "        ORDER BY clientId, event_date, visit_number ASC\n",
    "        '''\n",
    "\n",
    "        static_table = self.client.query(static_query).result().to_arrow(create_bqstorage_client=True).to_pandas()\n",
    "\n",
    "        return (static_table, static_table.shape)\n",
    "\n",
    "\n",
    "    # behavior query\n",
    "    # This queried table is the feature data updated with function \"update_pivot\" above\n",
    "    # For both \"history\" and \"realtime\" data (updated daily)\n",
    "    def behavior_query(self):\n",
    "        behavior_query = f'''\n",
    "        SELECT * FROM `<company data table on BigQuery: self.table>` \n",
    "        ORDER BY clientId, visitNumber, event_date, snapshot_date\n",
    "        '''\n",
    "        \n",
    "        behavior_table = self.client.query(behavior_query).result().to_arrow(create_bqstorage_client=True).to_pandas()\n",
    "\n",
    "        return (behavior_table, behavior_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return analysis\n",
    "Another customer website behavior created with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return Analysis: customer behavior for their last 2 website visits\n",
    "\n",
    "class Return_analysis:\n",
    "    \n",
    "    def __init__(self, static_df, type):\n",
    "        self.static_df = static_df\n",
    "        self.type = type\n",
    "    \n",
    "    # Data pre-processing\n",
    "    def pre_processing(self):\n",
    "        self.static_df['event_date'] = pd.to_datetime(self.static_df['event_date'], format='%Y-%m-%d')\n",
    "\n",
    "        visit_num = self.static_df.value_counts('clientId').reset_index()\n",
    "        print('# of total clientIds:', visit_num.shape[0])  # unique client num\n",
    "        only_once = visit_num[visit_num[0] == 1]['clientId'].to_list()\n",
    "        \n",
    "        for_preprocess = self.static_df.sort_values(['clientId', 'visit_number'], ascending=False).reset_index(drop=True)\n",
    "        for_preprocess.fillna(0, inplace=True)  \n",
    "\n",
    "        return (for_preprocess, only_once)\n",
    "\n",
    "    \n",
    "    # Return analysis for \"purchasers\"\n",
    "    def RA_purchasers(self, for_preprocess):\n",
    "\n",
    "        # Purchasers with at least 2 visits: can do return analysis\n",
    "        clients_purchased = for_preprocess[for_preprocess['purchase_amount'] > 0].value_counts('clientId').reset_index()['clientId'].to_list()\n",
    "        purchases = for_preprocess[for_preprocess['clientId'].isin(clients_purchased)].reset_index(drop=True)\n",
    "        print('# of data of clients who purchase:', purchases.shape[0])\n",
    "        print('# of total purchasers:', purchases.value_counts('clientId').shape[0])\n",
    "        \n",
    "        for_groupby = {'clientId': 'last', 'event_date': 'last', 'visit_number': 'last', 'medium': 'last', 'purchase_amount': 'last'}\n",
    "\n",
    "        # Check who can do return analysis and who can't\n",
    "        check_first_purchase = purchases.groupby('clientId', as_index=False).agg(for_groupby)\n",
    "        no_RA_id = check_first_purchase[check_first_purchase['purchase_amount'] > 0]['clientId'].to_list()  \n",
    "\n",
    "        # Drop clients who can't do return analysis\n",
    "        to_drop = purchases[purchases['clientId'].isin(no_RA_id)]  \n",
    "        purchases_RA = purchases[~purchases['clientId'].isin(no_RA_id)]\n",
    "        to_drop = to_drop.groupby('clientId', as_index=False).apply(lambda x: x.iloc[:-1]).reset_index(drop=True)  \n",
    "        purchases_RA.reset_index(drop=True)\n",
    "        purchases_RA = pd.concat([purchases_RA, to_drop]).reset_index(drop=True)  \n",
    "\n",
    "        # Select purchasers\n",
    "        latest_purchases = purchases_RA[(purchases_RA['purchase_amount'] > 0)]  \n",
    "        latest_purchases_idx = list(latest_purchases.index)\n",
    "        before_latest_purchases_idx = [idx + 1 for idx in latest_purchases_idx]\n",
    "        final_purchase_idx = sorted(latest_purchases_idx + before_latest_purchases_idx, reverse=False)\n",
    "        return_purchases = purchases_RA.iloc[final_purchase_idx, :]  \n",
    "\n",
    "        # Exclude exceptions (minority)\n",
    "        outliers = return_purchases.value_counts('clientId').reset_index()[return_purchases.value_counts('clientId').reset_index()[0] == 1]['clientId'].to_list()\n",
    "        return_purchases = return_purchases[~return_purchases['clientId'].isin(outliers)]\n",
    "        print('# of purchasers who does RA:', return_purchases.value_counts('clientId').shape[0])\n",
    "\n",
    "        # Calculate time period between 2 visits & last medium\n",
    "        return_purchases.reset_index(drop=True, inplace=True)\n",
    "        purchase_date = return_purchases[return_purchases.index%2 == 0][['clientId', 'event_date', 'visit_number','medium']]\n",
    "        pre_purchase_date = return_purchases[return_purchases.index%2 != 0][['clientId', 'event_date', 'visit_number', 'medium']]\n",
    "        purchase_date.reset_index(drop=True, inplace=True), pre_purchase_date.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        x_RA = pd.concat([purchase_date, pre_purchase_date], axis=1)\n",
    "        x_RA.columns = ['clientId', 'latest event date', 'latest visit num', 'lastest medium', 'clientId1', 'second latest event date', 'visit_number1', 'second latest medium']\n",
    "        x_RA.drop(['clientId1', 'visit_number1'], axis=1, inplace=True)\n",
    "        x_RA['days since last visit'] = (x_RA['latest event date'] - x_RA['second latest event date']).dt.days\n",
    "        \n",
    "        # Purchasers who can't do return analysis: purchased at first visit\n",
    "        x_RA_clients = x_RA.value_counts('clientId').reset_index()['clientId'].to_list()\n",
    "        latest_purchases1 = purchases[~purchases['clientId'].isin(x_RA_clients)]\n",
    "        latest_purchases1 = latest_purchases1.drop_duplicates('clientId', keep='last')  # only remain purchaser data\n",
    "        print('# of purchasers who cannot do RA: ', latest_purchases1.shape[0])\n",
    "\n",
    "        final_purchase_idx1 = list(latest_purchases1.index)\n",
    "        new_purchases = purchases.iloc[final_purchase_idx1, :]\n",
    "        x_RA1 = new_purchases[['clientId', 'event_date', 'visit_number','medium']]\n",
    "        x_RA1.columns = ['clientId', 'latest event date', 'latest visit num', 'lastest medium']\n",
    "\n",
    "        x_RA.reset_index(drop=True, inplace=True), x_RA1.reset_index(drop=True, inplace=True)\n",
    "        return_analysis = pd.concat([x_RA, x_RA1])\n",
    "\n",
    "        return return_analysis\n",
    "\n",
    "\n",
    "    # Return analysis for \"non-purchasers\" \n",
    "    def RA_non_purchasers(self, for_preprocess, only_once, RA_purchasers):\n",
    "\n",
    "        clients_purchased = for_preprocess[for_preprocess['purchase_amount'] > 0].value_counts('clientId').reset_index()['clientId'].to_list() \n",
    "        no_purchase = for_preprocess[~for_preprocess['clientId'].isin(clients_purchased)].reset_index(drop=True) \n",
    "        print('# of total data of all non-purchasers:', no_purchase.shape)\n",
    "\n",
    "        # non-purchasers who can do return analysis\n",
    "        latest_no_purchase = no_purchase[~no_purchase['clientId'].isin(only_once)]\n",
    "        print('# of data of non-purchasers who visit more than once:', latest_no_purchase.shape[0])\n",
    "\n",
    "        latest = latest_no_purchase.drop_duplicates('clientId', keep='first')\n",
    "        latest_index = list(latest.index)\n",
    "        latest.drop('purchase_amount', axis=1, inplace=True)\n",
    "        latest.columns = ['clientId', 'latest event date', 'latest visit num', 'lastest medium']\n",
    "\n",
    "        for_second_latest = latest_no_purchase.drop(latest_index)\n",
    "        second_latest = for_second_latest.drop_duplicates('clientId', keep='first')\n",
    "        second_latest.drop(['visit_number', 'purchase_amount'], axis=1, inplace=True)\n",
    "        second_latest.columns = ['clientId', 'second latest event date', 'second latest medium']\n",
    "\n",
    "        x_RA = latest.merge(second_latest, left_on='clientId', right_on='clientId', how='inner', right_index=False)\n",
    "        x_RA['days since last visit'] = (x_RA['latest event date'] - x_RA['second latest event date']).dt.days\n",
    "\n",
    "        # non-purchasers who can't do return analysis\n",
    "        latest_no_purchase1 = no_purchase[no_purchase['clientId'].isin(only_once)]\n",
    "        print('# of data of non-purchasers who only visit once:', latest_no_purchase1.shape[0])\n",
    "\n",
    "        x_RA1 = latest_no_purchase1[['clientId', 'event_date', 'visit_number','medium']]\n",
    "        x_RA1.columns = ['clientId', 'latest event date', 'latest visit num', 'lastest medium']\n",
    "\n",
    "        x_RA.reset_index(drop=True, inplace=True), x_RA1.reset_index(drop=True, inplace=True)\n",
    "        return_analysis = pd.concat([x_RA, x_RA1])\n",
    "\n",
    "\n",
    "        # history data: do sampling to control # of data\n",
    "        if self.type == 'history':\n",
    "            percentage = (RA_purchasers.value_counts('clientId').reset_index(drop=True).shape[0]/0.01)/return_analysis.shape[0]\n",
    "            print('training data sampling percentage: ' + str(round(percentage*100, 2)) + ' %')\n",
    "            new_return_analysis = return_analysis.sample(frac=percentage, random_state=0, axis=0).reset_index(drop=True)\n",
    "\n",
    "            print('# of total non-purchasers before sampling:', return_analysis.shape[0])\n",
    "            print('# of total non-purchasers after sampling: ', new_return_analysis.shape[0])\n",
    "            return new_return_analysis\n",
    "\n",
    "        else:\n",
    "            print('# of total non-purchasers: ', return_analysis.shape[0])\n",
    "            return return_analysis\n",
    "\n",
    "    \n",
    "    # Union purchaser and non-purchaser\n",
    "    def merge(self, RA_purchasers, RA_non_purchasers):\n",
    "        RA_purchasers.rename(columns={'visit_number_x': 'visit_number'}, inplace=True)\n",
    "        final_x_static_table = pd.concat([RA_non_purchasers, RA_purchasers])\n",
    "        final_x_static_table = final_x_static_table.sample(frac=1, random_state=0, axis=0).reset_index(drop=True) \n",
    "\n",
    "        final_x_static_table['medium combination'] = final_x_static_table['lastest medium'] + ' , ' + final_x_static_table['second latest medium']\n",
    "\n",
    "        # time delta --> days since last visit\n",
    "        final_x_static_table = final_x_static_table.rename({'time delta': 'days since last visit'})\n",
    "\n",
    "        final_x_static_table.sort_values('clientId', ascending=False, inplace=True)\n",
    "        \n",
    "        return final_x_static_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_cleansing:\n",
    "    \n",
    "    def __init__(self, static_df, behavior_df, type):\n",
    "        self.static_df = static_df\n",
    "        self.behavior_df = behavior_df\n",
    "        self.type = type\n",
    "\n",
    "\n",
    "    # Merge static data & behavior data + cleaning\n",
    "    def merge(self):\n",
    "        self.behavior_df['event_date'] = pd.to_datetime(self.behavior_df['event_date'], format='%Y%m%d')\n",
    "        x_pre_table = self.behavior_df.merge(self.static_df, left_on=['clientId', 'visitNumber'], right_on=['clientId', 'latest visit num'], how='inner')\n",
    "        print(x_pre_table.shape)\n",
    "\n",
    "        # Drop useless columns\n",
    "        x_pre_table.drop(['latest event date', 'second latest event date', 'time_delta', 'latest visit num'], axis=1, inplace=True)\n",
    "        print('# of data before dropping TimeOnSite < AVG rows:', x_pre_table.shape[0])\n",
    "\n",
    "        x_pre_table = x_pre_table[x_pre_table['Pageview'] > 1]\n",
    "        print('# of data after dropping Pageview <= 1 rows:', x_pre_table.shape[0])\n",
    "\n",
    "        # Drop 0 columns\n",
    "        print('# of columns before dropping all NaN columns:', x_pre_table.shape[1])\n",
    "        x_pre_table = x_pre_table.dropna(axis=1, how='all')\n",
    "        print('# of columns after dropping all NaN columns:', x_pre_table.shape[1])\n",
    "\n",
    "        # Address NaN\n",
    "        values = {'second latest medium': 'unknown', 'days since last visit': 180}\n",
    "        x_pre_table.fillna(value=values, inplace=True)\n",
    "\n",
    "        naming_dict = {'lastest medium': 'medium',  \n",
    "                    'second latest medium': 'medium1',}\n",
    "        x_pre_table.rename(columns=naming_dict, inplace=True)\n",
    "\n",
    "        x_pre_table.isna().sum()[x_pre_table.isna().sum() > 0].sort_values(ascending=False).to_frame(name='Num of NA') \n",
    "\n",
    "        # Data distribution\n",
    "        print(x_pre_table.shape)\n",
    "        x_pre_table.value_counts('label_transaction').head()\n",
    "\n",
    "        # Fill NaNs\n",
    "        x_pre_table.fillna(0, inplace=True)\n",
    "        x_pre_table.isna().sum()[x_pre_table.isna().sum() > 0].sort_values(ascending=False).to_frame(name='Num of NA')  # 沒有 NaN 了\n",
    "\n",
    "        # realtime data: Drop people who already purchased\n",
    "        if self.type == 'realtime':\n",
    "            purchasers = x_pre_table[x_pre_table['label_transaction'] > 0]['clientId'].tolist()\n",
    "            x_pre_table = x_pre_table[~x_pre_table['clientId'].isin(purchasers)]\n",
    "            print('# of purchasers dropped: ', len(purchasers))\n",
    "\n",
    "        return x_pre_table  # final table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_pre_processing:\n",
    "    \n",
    "    def __init__(self, x_pre_table, dummy_cols, type):\n",
    "\n",
    "        self.x_pre_table = x_pre_table[x_pre_table['days since last visit'] >= 0]\n",
    "        \n",
    "        self.x_pre_table['region'] = self.x_pre_table['region'].map({'Taiwan': 'region_Taiwan'}, na_action='ignore')\n",
    "        self.x_pre_table['country'] = self.x_pre_table['country'].map({'Taiwan': 'country__Taiwan'}, na_action='ignore')\n",
    "        \n",
    "        self.dummy_cols = dummy_cols\n",
    "        self.type = type\n",
    "\n",
    "    \n",
    "    # Get dummy variable & Drop useless columns\n",
    "    def get_dummy(self):\n",
    "        \n",
    "        # prepare data\n",
    "        clientId = self.x_pre_table['clientId'].reset_index(drop=True)\n",
    "        data = self.x_pre_table.drop(['clientId', 'snapshot_date', 'event_date', 'medium1', 'visitNumber'], axis=1)  \n",
    "\n",
    "        for column in self.dummy_cols:\n",
    "            dummy_df = pd.get_dummies(data[column])\n",
    "            for col in dummy_df.columns:\n",
    "                if dummy_df[col].sum() < self.x_pre_table.shape[0] * 0.01:  # Remove features with very few counts\n",
    "                    dummy_df.drop(col, axis=1, inplace=True)\n",
    "            data.reset_index(drop=True, inplace=True), dummy_df.reset_index(drop=True, inplace=True)\n",
    "            data = pd.concat([data, dummy_df], axis=1)\n",
    "            data.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "        data.drop(0, axis=1, inplace=True)  \n",
    "        data.drop('(not set)', axis=1, inplace=True)\n",
    "        y_data = data['label_transaction'].to_frame()\n",
    "        x_data = data.drop('label_transaction', axis=1)\n",
    "\n",
    "        if self.type == 'history':\n",
    "            return (x_data, y_data)  # Don't need to map client back to training (history) data\n",
    "        else:\n",
    "            return (x_data, y_data, clientId)  # Map clientId back to real-time data for advertising\n",
    "\n",
    "\n",
    "    # Cut data\n",
    "    def cut_data(self, X_data, y_data):\n",
    "        if self.type == 'history':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "            X_train, X_stack, y_train, y_stack = train_test_split(X_train, y_train, test_size=0.4, random_state=42)\n",
    "            print(X_train.shape)\n",
    "            print(y_train.shape)\n",
    "            print(X_test.shape)\n",
    "            print(y_test.shape)\n",
    "\n",
    "            return (X_train, y_train, X_test, y_test, X_stack, y_stack)\n",
    "        else:\n",
    "            print(X_data.shape)\n",
    "            print(y_data.shape)\n",
    "            \n",
    "            return(X_data, y_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "* Select the best model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different metrics\n",
    "\n",
    "class Model_evaluation:\n",
    "\n",
    "    def __init__(self, y_test, y_predict, y_proba):\n",
    "        self.y_test = y_test\n",
    "        self.y_predict = y_predict\n",
    "        self.y_proba = y_proba\n",
    "\n",
    "    # Balanced accuracy score\n",
    "    def balanced_acc(self):\n",
    "        accuracy = balanced_accuracy_score(self.y_test, self.y_predict)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    # AUC area\n",
    "    def AUC_area(self):\n",
    "        y_true = np.array(self.y_test)\n",
    "        y_score = np.array(self.y_proba)\n",
    "        roc_auc = roc_auc_score(y_true, y_score[:, 1])\n",
    "\n",
    "        return roc_auc\n",
    "\n",
    "\n",
    "    # Confusion matrix\n",
    "    def cf_matrix(self):\n",
    "        cf_matrix = confusion_matrix(self.y_test, self.y_predict)\n",
    "        # print(cf_matrix)\n",
    "        \n",
    "        ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "        ax.set_title('Confusion Matrix with labels\\n\\n')\n",
    "        ax.set_xlabel('\\nPredicted Values')\n",
    "        ax.set_ylabel('Actual Values ')\n",
    "\n",
    "        ## Ticket labels - List must be in alphabetical order\n",
    "        ax.xaxis.set_ticklabels(['False','True'])\n",
    "        ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "        ## Display the visualization of the Confusion Matrix.\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "* Model selection: history (training) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different models\n",
    "\n",
    "class Models:\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, X_stack, y_stack, type):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_stack = X_stack\n",
    "        self.y_stack = y_stack\n",
    "        self.type = type\n",
    "\n",
    "    \n",
    "    # gradient boosting\n",
    "    def gb(self):\n",
    "        boosting = GradientBoostingClassifier(max_depth=5).fit(self.X_train, self.y_train)\n",
    "        boosting_prob = boosting.predict_proba(self.X_test)\n",
    "        boosting_pred = boosting.predict(self.X_test)\n",
    "\n",
    "        if self.type == 'history':\n",
    "            return(boosting, boosting_prob, boosting_pred)\n",
    "        else:\n",
    "            return(boosting, boosting_pred)\n",
    "\n",
    "    \n",
    "    # random forest\n",
    "    def rf(self):\n",
    "        forest = RandomForestClassifier(max_depth=5).fit(self.X_train, self.y_train)\n",
    "        forest_prob = forest.predict_proba(self.X_test)\n",
    "        forest_pred = forest.predict(self.X_test)\n",
    "\n",
    "        if self.type == 'history':\n",
    "            return(forest, forest_prob, forest_pred)\n",
    "        else:\n",
    "            return(forest, forest_pred)\n",
    "\n",
    "\n",
    "\n",
    "    # XGBoost\n",
    "    def xgb(self):\n",
    "        xgboost = XGBClassifier(max_depth=5).fit(self.X_train, self.y_train)\n",
    "        xgboost_prob = xgboost.predict_proba(self.X_test)\n",
    "        xgboost_pred = xgboost.predict(self.X_test)\n",
    "\n",
    "        if self.type == 'history':\n",
    "            return(xgboost, xgboost_prob, xgboost_pred)\n",
    "        else:\n",
    "            return(xgboost, xgboost_pred)\n",
    "\n",
    "    \n",
    "    # Stacking\n",
    "    def stacking(self, boosting, forest, xgboost):\n",
    "        level_0 = [('Gradient Boosting', boosting), ('Random Forest', forest), ('XGBoost', xgboost)]   \n",
    "\n",
    "        level_1 = StackingClassifier(estimators=level_0, final_estimator=LogisticRegression())\n",
    "        stack = level_1.fit(self.X_stack, self.y_stack)\n",
    "\n",
    "        stack_prob = stack.predict_proba(self.X_test)\n",
    "        stack_pred = stack.predict(self.X_test)\n",
    "\n",
    "        if self.type == 'history':\n",
    "            return(stack_prob, stack_pred)\n",
    "        else:\n",
    "            return stack_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tune_models:\n",
    "\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    \n",
    "    def gb(self):\n",
    "        gb = GradientBoostingClassifier(random_state=42)\n",
    "        gb_param_grid = {'learning_rate': np.logspace(-3, 0, 10, endpoint=True),\n",
    "                        'n_estimators': [100, 300, 500],\n",
    "                        'max_depth': [1, 2, 3]}\n",
    "\n",
    "        search_gb = GridSearchCV(estimator=gb,\n",
    "                            param_grid=gb_param_grid,\n",
    "                            cv=5,\n",
    "                            scoring='balanced_accuracy',\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "\n",
    "        search_gb.fit(self.X_train, self.y_train)\n",
    "\n",
    "        return(search_gb.best_score_, search_gb.best_estimator_)\n",
    "\n",
    "\n",
    "    def rf(self):\n",
    "        rf = RandomForestClassifier(random_state=42)\n",
    "        rf_param_grid = {'n_estimators': [100, 300, 500],\n",
    "                        'criterion': ['gini', 'entropy'],\n",
    "                        'max_depth': [10, 20, 30],\n",
    "                        'max_features': ['sqrt', 0.6, 0.7, 0.8, None]}\n",
    "\n",
    "        search_rf = GridSearchCV(estimator=rf,\n",
    "                            param_grid=rf_param_grid,\n",
    "                            cv=5,\n",
    "                            scoring='balanced_accuracy',\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "\n",
    "        search_rf.fit(self.X_train, self.y_train)\n",
    "\n",
    "        return(search_rf.best_score_, search_rf.best_estimator_)\n",
    "\n",
    "\n",
    "    def xgb(self):\n",
    "        xgb = XGBClassifier()\n",
    "        xgb_param_grid = {'learning_rate': np.logspace(-3, 0, 10, endpoint=True),\n",
    "                        'max_depth': [3, 5, 8],  # 不能太高\n",
    "                        'min_child_weight': [1, 3, 5],  # 不能太低\n",
    "                        'gamma': [0, 0.05, 0.1],  \n",
    "                        'colsample_bytree': [0.5, 0.7, 0.9]}\n",
    "\n",
    "        search_xgb = GridSearchCV(estimator=xgb,\n",
    "                            param_grid=xgb_param_grid,\n",
    "                            cv=5,\n",
    "                            scoring='balanced_accuracy',\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "\n",
    "        search_xgb.fit(self.X_train, self.y_train)\n",
    "\n",
    "        return (search_xgb.best_score_, search_xgb.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output & Relabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Export_output:\n",
    "\n",
    "    def __init__(self, model_prob, clientId, file_name, snapshot_date):\n",
    "        self.model_prob = model_prob\n",
    "        self.clientId = clientId\n",
    "        self.file_name = file_name\n",
    "        self.snapshot_date = snapshot_date\n",
    "\n",
    "\n",
    "    # Create an empty csv file to write in the re-scored outputs to upload\n",
    "    def create_csv(self):\n",
    "\n",
    "        df = pd.DataFrame(list())\n",
    "        df.to_csv(self.file_name + '.csv')\n",
    "        \n",
    "        return self.file_name + '.csv'  # For API file location\n",
    "\n",
    "\n",
    "    # Create an empty excel file to write in the original scores of outputs\n",
    "    def create_excel(self):\n",
    "        workbook = Workbook()\n",
    "        workbook.save(self.file_name + '.xlsx')\n",
    "\n",
    "\n",
    "    def output(self, n):  # n: the criteria for \"next purchasers\"\n",
    "        propensity = [i[1] for i in self.model_prob.tolist()]\n",
    "        propensity_df = pd.DataFrame(propensity, columns=['purchase propensity'])\n",
    "        clientId_df = self.clientId.to_frame().reset_index(drop=True)\n",
    "        output = pd.concat([clientId_df, propensity_df], axis=1)\n",
    "        output = output.sort_values('purchase propensity', ascending=False)\n",
    "        output.reset_index(inplace=True, drop=True)\n",
    "\n",
    "       # Next purchasers\n",
    "        n_num = output.shape[0] * n\n",
    "        n_score = output.loc[int(n_num), 'purchase propensity']\n",
    "        print('# of high propensity customers:', n_num)\n",
    "        output['relabeled propensity'] = output['purchase propensity'].apply(lambda x: 0.16 if x >= n_score else 0)\n",
    "        output.to_excel(self.file_name + '.xlsx')\n",
    "\n",
    "        output_new = output[['clientId', 'relabeled propensity']]\n",
    "        output_new = output_new[output_new['relabeled propensity'] > 0]\n",
    "        output_new['relabeled propensity_'] = output_new['relabeled propensity'].astype(str)\n",
    "        \n",
    "        output_new['relabeled propensity_'] = output_new['relabeled propensity_'] + self.snapshot_date\n",
    "        output_new.drop('relabeled propensity', axis=1, inplace=True)\n",
    "        output_new['relabeled propensity_'] = output_new['relabeled propensity_'].astype(float)\n",
    "        \n",
    "        print('# of high propensity leads:', output_new.shape)\n",
    "        output_new.to_csv(self.file_name + '.csv', index=False)\n",
    "        \n",
    "        return output_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / history data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation + Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update realtime data before import any data\n",
    "realtime_raw_data = '<company data table on BigQuery>'\n",
    "\n",
    "update_data = Update_data(realtime_raw_data)\n",
    "# update_data.update_raw()\n",
    "update_data.update_pivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from GBQ\n",
    "t_static_data = '<company data table on BigQuery>'\n",
    "t_behavior_data = '<company data table on BigQuery>'\n",
    "\n",
    "t_queries = Queries(t_static_data, t_behavior_data)\n",
    "t_static, t_static_shape = t_queries.static_query()\n",
    "t_behavior, t_behavior_shape = t_queries.behavior_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_RA = Return_analysis(t_static, 'history')\n",
    "t_static, t_only_once = t_RA.pre_processing()\n",
    "print('# of clientIds only visit once:', len(t_only_once)) \n",
    "print('# of total static data:', t_static.shape)\n",
    "\n",
    "t_RA_purchasers = t_RA.RA_purchasers(t_static)  # purchasers\n",
    "print('# of total purchasers: ', t_RA_purchasers.value_counts('clientId').shape[0])\n",
    "\n",
    "t_RA_non_purchasers = t_RA.RA_non_purchasers(t_static, t_only_once, t_RA_purchasers)  # non-purchasers\n",
    "\n",
    "# merge purchasers & non-purchasers\n",
    "t_final_static_table = t_RA.merge(t_RA_purchasers, t_RA_non_purchasers)\n",
    "print(t_final_static_table.shape)\n",
    "print('# of total RA clients:', t_final_static_table.value_counts('clientId').shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleansing\n",
    "t_data_cleansing = Data_cleansing(t_final_static_table, t_behavior, 'history')\n",
    "t_pre_table = t_data_cleansing.merge()  # final table before data pre-processing\n",
    "t_pre_table.isna().sum()[t_pre_table.isna().sum() > 0].sort_values(ascending=False).to_frame(name='Num of NA')  \n",
    "\n",
    "\n",
    "# Data pre-processing\n",
    "dummy_cols = ['medium', 'medium combination', 'search_keyword', 'browser', 'is_mobile', 'mobile_brand', 'device_category', 'country', 'region']\n",
    "t_data_preprocess = Data_pre_processing(t_pre_table, dummy_cols, 'history')\n",
    "t_x_data, t_y_data = t_data_preprocess.get_dummy()\n",
    "X_train, y_train, X_test, y_test, X_stack, y_stack =  t_data_preprocess.cut_data(t_x_data, t_y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train models + Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train diffferent models\n",
    "train_models = Models(X_train, y_train, X_test, y_test, X_stack, y_stack, 'history')\n",
    "boosting, gb_y_prob, gb_y_pred = train_models.gb()\n",
    "forest, rf_y_prob, rf_y_pred = train_models.rf()\n",
    "xgboost, xgb_y_prob, xgb_y_pred = train_models.xgb()\n",
    "stack_y_prob, stack_y_pred = train_models.stacking(boosting, forest, xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation: select gradient boosting as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting\n",
    "gb_evaluation = Model_evaluation(y_test, gb_y_pred, gb_y_prob)\n",
    "gb_roc = gb_evaluation.AUC_area()\n",
    "print('Gradient boosting AUC area: ', gb_roc) \n",
    "print('Gradient boosting balanced accuracy: ', gb_evaluation.balanced_acc())  # Gradient boosting balanced accuracy: 0.8339378395448561\n",
    "gb_evaluation.cf_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "rf_evaluation = Model_evaluation(y_test, rf_y_pred, rf_y_prob)\n",
    "rf_roc = rf_evaluation.AUC_area()\n",
    "print('Random forest AUC area: ', rf_roc)\n",
    "print('Random forest balanced accuracy score: ', rf_evaluation.balanced_acc())  # Random forest balanced accuracy score:  0.6649750991427331\n",
    "rf_evaluation.cf_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_evaluation = Model_evaluation(y_test, xgb_y_pred, xgb_y_prob)\n",
    "xgb_roc = xgb_evaluation.AUC_area()\n",
    "print('XGBoost AUC area: ', xgb_roc)\n",
    "print('XGBoost balanced accuracy score: ', xgb_evaluation.balanced_acc())  # XGBoost balanced accuracy score:  0.836694660577354\n",
    "xgb_evaluation.cf_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking\n",
    "stack_evaluation = Model_evaluation(y_test, stack_y_pred, stack_y_prob)\n",
    "stack_roc = stack_evaluation.AUC_area()\n",
    "print('Stacking AUC area: ', stack_roc)\n",
    "print('Stacking accuracy score: ', stack_evaluation.balanced_acc())  # Stacking accuracy score:  0.8357770498754344\n",
    "stack_evaluation.cf_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Gradient Boosting in this campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realtime data: X_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address feature difference\n",
    "def discard_diff(X_train, X_test):\n",
    "    diff_cols = list(set(X_train.columns).symmetric_difference(set(X_test.columns)))\n",
    "    X_train.drop(diff_cols, axis=1, inplace=True, errors='ignore')\n",
    "    X_test.drop(diff_cols, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the prediction of the best model\n",
    "def output(model_prob, clientId):\n",
    "    propensity = [i[1] for i in model_prob.tolist()]\n",
    "    propensity_df = pd.DataFrame(propensity, columns=['purchase propensity'])\n",
    "\n",
    "    clientId_df = clientId.to_frame().reset_index(drop=True)\n",
    "    output = pd.concat([clientId_df, propensity_df], axis=1)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation + Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from GBQ\n",
    "r_static_data = '<company data table on BigQuery>'\n",
    "r_behavior_data = '<company data table on BigQuery>'\n",
    "\n",
    "r_queries = Queries(r_static_data, r_behavior_data)\n",
    "r_static, r_static_shape = r_queries.static_query()\n",
    "r_behavior, r_behavior_shape = r_queries.behavior_query()\n",
    "\n",
    "# Check if the data is updated\n",
    "print('behavior data max date: ', r_behavior['event_date'].max())\n",
    "print('behavior data min date: ', r_behavior['event_date'].min())\n",
    "print('behavior data snapshot date: ', r_behavior['snapshot_date'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return analysis\n",
    "r_RA = Return_analysis(r_static, 'realtime')\n",
    "r_static, r_only_once = r_RA.pre_processing()\n",
    "print('# of clientIds only visit once:', len(r_only_once)) \n",
    "print('# of total static data:', r_static.shape)\n",
    "\n",
    "r_RA_purchasers = r_RA.RA_purchasers(r_static)  # purchasers\n",
    "print('# of total purchasers: ', r_RA_purchasers.value_counts('clientId').shape[0])\n",
    "\n",
    "r_RA_non_purchasers = r_RA.RA_non_purchasers(r_static, r_only_once, r_RA_purchasers)  # non-purchasers\n",
    "\n",
    "# merge purchasers & non-purchasers\n",
    "r_final_static_table = r_RA.merge(r_RA_purchasers, r_RA_non_purchasers)\n",
    "print(r_final_static_table.shape)\n",
    "print('# of total RA clients:', r_final_static_table.value_counts('clientId').shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleansing\n",
    "r_data_cleansing = Data_cleansing(r_final_static_table, r_behavior, 'realtime')\n",
    "r_pre_table = r_data_cleansing.merge()  # final table before data pre-processing\n",
    "r_pre_table.isna().sum()[r_pre_table.isna().sum() > 0].sort_values(ascending=False).to_frame(name='Num of NA')  \n",
    "\n",
    "# Data pre-processing\n",
    "r_data_preprocess = Data_pre_processing(r_pre_table, dummy_cols, 'realtime')\n",
    "r_x_data, r_y_data, clientId = r_data_preprocess.get_dummy()\n",
    "X_real, y_real =  r_data_preprocess.cut_data(r_x_data, r_y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the datasets as final training data\n",
    "X_train_new = pd.concat([X_train, X_test, X_stack])\n",
    "y_train_new = pd.concat([y_train, y_test, y_stack])\n",
    "X_train_new, X_real = discard_diff(X_train_new, X_real)\n",
    "print(X_train_new.shape)\n",
    "print(y_train_new.shape)\n",
    "print(X_real.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict with the selected model + Export output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose gradient boosting in this campaign\n",
    "boosting_real = GradientBoostingClassifier(max_depth=5).fit(X_train_new, y_train_new)\n",
    "boosting_prob_real = boosting_real.predict_proba(X_real)\n",
    "boosting_pred_real = boosting_real.predict(X_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting feature importance\n",
    "features = list(X_train_new.columns)\n",
    "boosting_real_feat = boosting_real.feature_importances_\n",
    "boosting_real_feat_df = pd.DataFrame({'features': features}).join(pd.DataFrame({'weights': boosting_real_feat}))\n",
    "gb_feat_imp_real = boosting_real_feat_df.sort_values('weights', ascending=False)\n",
    "gb_feat_imp_real.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output & Output Relabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "yesterday = today - timedelta(days = 1)\n",
    "snapshot_date = str(yesterday.strftime('%m%d'))\n",
    "file_name = 'client_propensity_' + snapshot_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update automatically every day\n",
    "today = date.today()\n",
    "yesterday = today - timedelta(days = 1)\n",
    "snpshot_date = str(yesterday.strftime('%m%d'))\n",
    "print('snapshot date: ', snapshot_date)\n",
    "\n",
    "output_gb = Export_output(boosting_prob_real, clientId, file_name, snapshot_date)\n",
    "csv_location_name = output_gb.create_csv()  \n",
    "output_gb.create_excel()\n",
    "output_new = output_gb.output()\n",
    "\n",
    "output_new.head()  # Predicion result: Upload and Check with \"Management api\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
